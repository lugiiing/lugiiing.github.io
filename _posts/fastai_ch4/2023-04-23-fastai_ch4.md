---
layout: post
title: "fastai_ch1"
---



```python
!pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
```


```python
from fastbook import *
```


```python
from fastai.vision.all import *
matplotlib.rc('image', cmap='Greys')
```

Werbos?


```python
path = untar_data(URLs.MNIST_SAMPLE)
Path.BASE_PATH = path
```


```python
path.ls()
```




    (#3) [Path('labels.csv'),Path('train'),Path('valid')]




```python
(path/'train').ls()
```




    (#2) [Path('train/3'),Path('train/7')]




```python
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
threes
```



```python
im3_path = threes[1]
im3 = Image.open(im3_path)
im3
```




    
![png](/../../images/fastai_ch4/output_8_0.png)
    




```python
array(im3)[4:10,4:10]
```




    array([[  0,   0,   0,   0,   0,   0],
           [  0,   0,   0,   0,   0,  29],
           [  0,   0,   0,  48, 166, 224],
           [  0,  93, 244, 249, 253, 187],
           [  0, 107, 253, 253, 230,  48],
           [  0,   3,  20,  20,  15,   0]], dtype=uint8)




```python
tensor(im3)[4:10,4:10]
```




    tensor([[  0,   0,   0,   0,   0,   0],
            [  0,   0,   0,   0,   0,  29],
            [  0,   0,   0,  48, 166, 224],
            [  0,  93, 244, 249, 253, 187],
            [  0, 107, 253, 253, 230,  48],
            [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)




```python

im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:15,4:22])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
     
```




<style type="text/css">
#T_d17b9_row0_col0, #T_d17b9_row0_col1, #T_d17b9_row0_col2, #T_d17b9_row0_col3, #T_d17b9_row0_col4, #T_d17b9_row0_col5, #T_d17b9_row0_col6, #T_d17b9_row0_col7, #T_d17b9_row0_col8, #T_d17b9_row0_col9, #T_d17b9_row0_col10, #T_d17b9_row0_col11, #T_d17b9_row0_col12, #T_d17b9_row0_col13, #T_d17b9_row0_col14, #T_d17b9_row0_col15, #T_d17b9_row0_col16, #T_d17b9_row0_col17, #T_d17b9_row1_col0, #T_d17b9_row1_col1, #T_d17b9_row1_col2, #T_d17b9_row1_col3, #T_d17b9_row1_col4, #T_d17b9_row1_col15, #T_d17b9_row1_col16, #T_d17b9_row1_col17, #T_d17b9_row2_col0, #T_d17b9_row2_col1, #T_d17b9_row2_col2, #T_d17b9_row2_col15, #T_d17b9_row2_col16, #T_d17b9_row2_col17, #T_d17b9_row3_col0, #T_d17b9_row3_col15, #T_d17b9_row3_col16, #T_d17b9_row3_col17, #T_d17b9_row4_col0, #T_d17b9_row4_col6, #T_d17b9_row4_col7, #T_d17b9_row4_col8, #T_d17b9_row4_col9, #T_d17b9_row4_col10, #T_d17b9_row4_col15, #T_d17b9_row4_col16, #T_d17b9_row4_col17, #T_d17b9_row5_col0, #T_d17b9_row5_col5, #T_d17b9_row5_col6, #T_d17b9_row5_col7, #T_d17b9_row5_col8, #T_d17b9_row5_col9, #T_d17b9_row5_col15, #T_d17b9_row5_col16, #T_d17b9_row5_col17, #T_d17b9_row6_col0, #T_d17b9_row6_col1, #T_d17b9_row6_col2, #T_d17b9_row6_col3, #T_d17b9_row6_col4, #T_d17b9_row6_col5, #T_d17b9_row6_col6, #T_d17b9_row6_col7, #T_d17b9_row6_col8, #T_d17b9_row6_col9, #T_d17b9_row6_col14, #T_d17b9_row6_col15, #T_d17b9_row6_col16, #T_d17b9_row6_col17, #T_d17b9_row7_col0, #T_d17b9_row7_col1, #T_d17b9_row7_col2, #T_d17b9_row7_col3, #T_d17b9_row7_col4, #T_d17b9_row7_col5, #T_d17b9_row7_col6, #T_d17b9_row7_col13, #T_d17b9_row7_col14, #T_d17b9_row7_col15, #T_d17b9_row7_col16, #T_d17b9_row7_col17, #T_d17b9_row8_col0, #T_d17b9_row8_col1, #T_d17b9_row8_col2, #T_d17b9_row8_col3, #T_d17b9_row8_col4, #T_d17b9_row8_col13, #T_d17b9_row8_col14, #T_d17b9_row8_col15, #T_d17b9_row8_col16, #T_d17b9_row8_col17, #T_d17b9_row9_col0, #T_d17b9_row9_col1, #T_d17b9_row9_col2, #T_d17b9_row9_col3, #T_d17b9_row9_col4, #T_d17b9_row9_col16, #T_d17b9_row9_col17, #T_d17b9_row10_col0, #T_d17b9_row10_col1, #T_d17b9_row10_col2, #T_d17b9_row10_col3, #T_d17b9_row10_col4, #T_d17b9_row10_col5, #T_d17b9_row10_col6, #T_d17b9_row10_col17 {
  font-size: 6pt;
  background-color: #ffffff;
  color: #000000;
}
#T_d17b9_row1_col5 {
  font-size: 6pt;
  background-color: #efefef;
  color: #000000;
}
#T_d17b9_row1_col6, #T_d17b9_row1_col13 {
  font-size: 6pt;
  background-color: #7c7c7c;
  color: #f1f1f1;
}
#T_d17b9_row1_col7 {
  font-size: 6pt;
  background-color: #4a4a4a;
  color: #f1f1f1;
}
#T_d17b9_row1_col8, #T_d17b9_row1_col9, #T_d17b9_row1_col10, #T_d17b9_row2_col5, #T_d17b9_row2_col6, #T_d17b9_row2_col7, #T_d17b9_row2_col11, #T_d17b9_row2_col12, #T_d17b9_row2_col13, #T_d17b9_row3_col4, #T_d17b9_row3_col12, #T_d17b9_row3_col13, #T_d17b9_row4_col1, #T_d17b9_row4_col2, #T_d17b9_row4_col3, #T_d17b9_row4_col12, #T_d17b9_row4_col13, #T_d17b9_row5_col12, #T_d17b9_row6_col11, #T_d17b9_row9_col11, #T_d17b9_row10_col11, #T_d17b9_row10_col12, #T_d17b9_row10_col13, #T_d17b9_row10_col14, #T_d17b9_row10_col15, #T_d17b9_row10_col16 {
  font-size: 6pt;
  background-color: #000000;
  color: #f1f1f1;
}
#T_d17b9_row1_col11 {
  font-size: 6pt;
  background-color: #606060;
  color: #f1f1f1;
}
#T_d17b9_row1_col12 {
  font-size: 6pt;
  background-color: #4d4d4d;
  color: #f1f1f1;
}
#T_d17b9_row1_col14 {
  font-size: 6pt;
  background-color: #bbbbbb;
  color: #000000;
}
#T_d17b9_row2_col3 {
  font-size: 6pt;
  background-color: #e4e4e4;
  color: #000000;
}
#T_d17b9_row2_col4, #T_d17b9_row8_col6 {
  font-size: 6pt;
  background-color: #6b6b6b;
  color: #f1f1f1;
}
#T_d17b9_row2_col8, #T_d17b9_row2_col14, #T_d17b9_row3_col14 {
  font-size: 6pt;
  background-color: #171717;
  color: #f1f1f1;
}
#T_d17b9_row2_col9, #T_d17b9_row3_col11 {
  font-size: 6pt;
  background-color: #4b4b4b;
  color: #f1f1f1;
}
#T_d17b9_row2_col10, #T_d17b9_row7_col10, #T_d17b9_row8_col8, #T_d17b9_row8_col10, #T_d17b9_row9_col8, #T_d17b9_row9_col10 {
  font-size: 6pt;
  background-color: #010101;
  color: #f1f1f1;
}
#T_d17b9_row3_col1 {
  font-size: 6pt;
  background-color: #272727;
  color: #f1f1f1;
}
#T_d17b9_row3_col2 {
  font-size: 6pt;
  background-color: #0a0a0a;
  color: #f1f1f1;
}
#T_d17b9_row3_col3 {
  font-size: 6pt;
  background-color: #050505;
  color: #f1f1f1;
}
#T_d17b9_row3_col5 {
  font-size: 6pt;
  background-color: #333333;
  color: #f1f1f1;
}
#T_d17b9_row3_col6 {
  font-size: 6pt;
  background-color: #e6e6e6;
  color: #000000;
}
#T_d17b9_row3_col7, #T_d17b9_row3_col10 {
  font-size: 6pt;
  background-color: #fafafa;
  color: #000000;
}
#T_d17b9_row3_col8 {
  font-size: 6pt;
  background-color: #fbfbfb;
  color: #000000;
}
#T_d17b9_row3_col9 {
  font-size: 6pt;
  background-color: #fdfdfd;
  color: #000000;
}
#T_d17b9_row4_col4 {
  font-size: 6pt;
  background-color: #1b1b1b;
  color: #f1f1f1;
}
#T_d17b9_row4_col5 {
  font-size: 6pt;
  background-color: #e0e0e0;
  color: #000000;
}
#T_d17b9_row4_col11 {
  font-size: 6pt;
  background-color: #4e4e4e;
  color: #f1f1f1;
}
#T_d17b9_row4_col14 {
  font-size: 6pt;
  background-color: #767676;
  color: #f1f1f1;
}
#T_d17b9_row5_col1 {
  font-size: 6pt;
  background-color: #fcfcfc;
  color: #000000;
}
#T_d17b9_row5_col2, #T_d17b9_row5_col3 {
  font-size: 6pt;
  background-color: #f6f6f6;
  color: #000000;
}
#T_d17b9_row5_col4, #T_d17b9_row7_col7 {
  font-size: 6pt;
  background-color: #f8f8f8;
  color: #000000;
}
#T_d17b9_row5_col10, #T_d17b9_row10_col7 {
  font-size: 6pt;
  background-color: #e8e8e8;
  color: #000000;
}
#T_d17b9_row5_col11 {
  font-size: 6pt;
  background-color: #222222;
  color: #f1f1f1;
}
#T_d17b9_row5_col13, #T_d17b9_row6_col12 {
  font-size: 6pt;
  background-color: #090909;
  color: #f1f1f1;
}
#T_d17b9_row5_col14 {
  font-size: 6pt;
  background-color: #d0d0d0;
  color: #000000;
}
#T_d17b9_row6_col10, #T_d17b9_row7_col11, #T_d17b9_row9_col6 {
  font-size: 6pt;
  background-color: #060606;
  color: #f1f1f1;
}
#T_d17b9_row6_col13 {
  font-size: 6pt;
  background-color: #979797;
  color: #f1f1f1;
}
#T_d17b9_row7_col8 {
  font-size: 6pt;
  background-color: #b6b6b6;
  color: #000000;
}
#T_d17b9_row7_col9 {
  font-size: 6pt;
  background-color: #252525;
  color: #f1f1f1;
}
#T_d17b9_row7_col12 {
  font-size: 6pt;
  background-color: #999999;
  color: #f1f1f1;
}
#T_d17b9_row8_col5 {
  font-size: 6pt;
  background-color: #f9f9f9;
  color: #000000;
}
#T_d17b9_row8_col7 {
  font-size: 6pt;
  background-color: #101010;
  color: #f1f1f1;
}
#T_d17b9_row8_col9, #T_d17b9_row9_col9 {
  font-size: 6pt;
  background-color: #020202;
  color: #f1f1f1;
}
#T_d17b9_row8_col11 {
  font-size: 6pt;
  background-color: #545454;
  color: #f1f1f1;
}
#T_d17b9_row8_col12 {
  font-size: 6pt;
  background-color: #f1f1f1;
  color: #000000;
}
#T_d17b9_row9_col5 {
  font-size: 6pt;
  background-color: #f7f7f7;
  color: #000000;
}
#T_d17b9_row9_col7 {
  font-size: 6pt;
  background-color: #030303;
  color: #f1f1f1;
}
#T_d17b9_row9_col12 {
  font-size: 6pt;
  background-color: #181818;
  color: #f1f1f1;
}
#T_d17b9_row9_col13 {
  font-size: 6pt;
  background-color: #303030;
  color: #f1f1f1;
}
#T_d17b9_row9_col14 {
  font-size: 6pt;
  background-color: #a9a9a9;
  color: #f1f1f1;
}
#T_d17b9_row9_col15 {
  font-size: 6pt;
  background-color: #fefefe;
  color: #000000;
}
#T_d17b9_row10_col8, #T_d17b9_row10_col9 {
  font-size: 6pt;
  background-color: #bababa;
  color: #000000;
}
#T_d17b9_row10_col10 {
  font-size: 6pt;
  background-color: #393939;
  color: #f1f1f1;
}
</style>
<table id="T_d17b9">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_d17b9_level0_col0" class="col_heading level0 col0" >0</th>
      <th id="T_d17b9_level0_col1" class="col_heading level0 col1" >1</th>
      <th id="T_d17b9_level0_col2" class="col_heading level0 col2" >2</th>
      <th id="T_d17b9_level0_col3" class="col_heading level0 col3" >3</th>
      <th id="T_d17b9_level0_col4" class="col_heading level0 col4" >4</th>
      <th id="T_d17b9_level0_col5" class="col_heading level0 col5" >5</th>
      <th id="T_d17b9_level0_col6" class="col_heading level0 col6" >6</th>
      <th id="T_d17b9_level0_col7" class="col_heading level0 col7" >7</th>
      <th id="T_d17b9_level0_col8" class="col_heading level0 col8" >8</th>
      <th id="T_d17b9_level0_col9" class="col_heading level0 col9" >9</th>
      <th id="T_d17b9_level0_col10" class="col_heading level0 col10" >10</th>
      <th id="T_d17b9_level0_col11" class="col_heading level0 col11" >11</th>
      <th id="T_d17b9_level0_col12" class="col_heading level0 col12" >12</th>
      <th id="T_d17b9_level0_col13" class="col_heading level0 col13" >13</th>
      <th id="T_d17b9_level0_col14" class="col_heading level0 col14" >14</th>
      <th id="T_d17b9_level0_col15" class="col_heading level0 col15" >15</th>
      <th id="T_d17b9_level0_col16" class="col_heading level0 col16" >16</th>
      <th id="T_d17b9_level0_col17" class="col_heading level0 col17" >17</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_d17b9_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_d17b9_row0_col0" class="data row0 col0" >0</td>
      <td id="T_d17b9_row0_col1" class="data row0 col1" >0</td>
      <td id="T_d17b9_row0_col2" class="data row0 col2" >0</td>
      <td id="T_d17b9_row0_col3" class="data row0 col3" >0</td>
      <td id="T_d17b9_row0_col4" class="data row0 col4" >0</td>
      <td id="T_d17b9_row0_col5" class="data row0 col5" >0</td>
      <td id="T_d17b9_row0_col6" class="data row0 col6" >0</td>
      <td id="T_d17b9_row0_col7" class="data row0 col7" >0</td>
      <td id="T_d17b9_row0_col8" class="data row0 col8" >0</td>
      <td id="T_d17b9_row0_col9" class="data row0 col9" >0</td>
      <td id="T_d17b9_row0_col10" class="data row0 col10" >0</td>
      <td id="T_d17b9_row0_col11" class="data row0 col11" >0</td>
      <td id="T_d17b9_row0_col12" class="data row0 col12" >0</td>
      <td id="T_d17b9_row0_col13" class="data row0 col13" >0</td>
      <td id="T_d17b9_row0_col14" class="data row0 col14" >0</td>
      <td id="T_d17b9_row0_col15" class="data row0 col15" >0</td>
      <td id="T_d17b9_row0_col16" class="data row0 col16" >0</td>
      <td id="T_d17b9_row0_col17" class="data row0 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_d17b9_row1_col0" class="data row1 col0" >0</td>
      <td id="T_d17b9_row1_col1" class="data row1 col1" >0</td>
      <td id="T_d17b9_row1_col2" class="data row1 col2" >0</td>
      <td id="T_d17b9_row1_col3" class="data row1 col3" >0</td>
      <td id="T_d17b9_row1_col4" class="data row1 col4" >0</td>
      <td id="T_d17b9_row1_col5" class="data row1 col5" >29</td>
      <td id="T_d17b9_row1_col6" class="data row1 col6" >150</td>
      <td id="T_d17b9_row1_col7" class="data row1 col7" >195</td>
      <td id="T_d17b9_row1_col8" class="data row1 col8" >254</td>
      <td id="T_d17b9_row1_col9" class="data row1 col9" >255</td>
      <td id="T_d17b9_row1_col10" class="data row1 col10" >254</td>
      <td id="T_d17b9_row1_col11" class="data row1 col11" >176</td>
      <td id="T_d17b9_row1_col12" class="data row1 col12" >193</td>
      <td id="T_d17b9_row1_col13" class="data row1 col13" >150</td>
      <td id="T_d17b9_row1_col14" class="data row1 col14" >96</td>
      <td id="T_d17b9_row1_col15" class="data row1 col15" >0</td>
      <td id="T_d17b9_row1_col16" class="data row1 col16" >0</td>
      <td id="T_d17b9_row1_col17" class="data row1 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_d17b9_row2_col0" class="data row2 col0" >0</td>
      <td id="T_d17b9_row2_col1" class="data row2 col1" >0</td>
      <td id="T_d17b9_row2_col2" class="data row2 col2" >0</td>
      <td id="T_d17b9_row2_col3" class="data row2 col3" >48</td>
      <td id="T_d17b9_row2_col4" class="data row2 col4" >166</td>
      <td id="T_d17b9_row2_col5" class="data row2 col5" >224</td>
      <td id="T_d17b9_row2_col6" class="data row2 col6" >253</td>
      <td id="T_d17b9_row2_col7" class="data row2 col7" >253</td>
      <td id="T_d17b9_row2_col8" class="data row2 col8" >234</td>
      <td id="T_d17b9_row2_col9" class="data row2 col9" >196</td>
      <td id="T_d17b9_row2_col10" class="data row2 col10" >253</td>
      <td id="T_d17b9_row2_col11" class="data row2 col11" >253</td>
      <td id="T_d17b9_row2_col12" class="data row2 col12" >253</td>
      <td id="T_d17b9_row2_col13" class="data row2 col13" >253</td>
      <td id="T_d17b9_row2_col14" class="data row2 col14" >233</td>
      <td id="T_d17b9_row2_col15" class="data row2 col15" >0</td>
      <td id="T_d17b9_row2_col16" class="data row2 col16" >0</td>
      <td id="T_d17b9_row2_col17" class="data row2 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_d17b9_row3_col0" class="data row3 col0" >0</td>
      <td id="T_d17b9_row3_col1" class="data row3 col1" >93</td>
      <td id="T_d17b9_row3_col2" class="data row3 col2" >244</td>
      <td id="T_d17b9_row3_col3" class="data row3 col3" >249</td>
      <td id="T_d17b9_row3_col4" class="data row3 col4" >253</td>
      <td id="T_d17b9_row3_col5" class="data row3 col5" >187</td>
      <td id="T_d17b9_row3_col6" class="data row3 col6" >46</td>
      <td id="T_d17b9_row3_col7" class="data row3 col7" >10</td>
      <td id="T_d17b9_row3_col8" class="data row3 col8" >8</td>
      <td id="T_d17b9_row3_col9" class="data row3 col9" >4</td>
      <td id="T_d17b9_row3_col10" class="data row3 col10" >10</td>
      <td id="T_d17b9_row3_col11" class="data row3 col11" >194</td>
      <td id="T_d17b9_row3_col12" class="data row3 col12" >253</td>
      <td id="T_d17b9_row3_col13" class="data row3 col13" >253</td>
      <td id="T_d17b9_row3_col14" class="data row3 col14" >233</td>
      <td id="T_d17b9_row3_col15" class="data row3 col15" >0</td>
      <td id="T_d17b9_row3_col16" class="data row3 col16" >0</td>
      <td id="T_d17b9_row3_col17" class="data row3 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_d17b9_row4_col0" class="data row4 col0" >0</td>
      <td id="T_d17b9_row4_col1" class="data row4 col1" >107</td>
      <td id="T_d17b9_row4_col2" class="data row4 col2" >253</td>
      <td id="T_d17b9_row4_col3" class="data row4 col3" >253</td>
      <td id="T_d17b9_row4_col4" class="data row4 col4" >230</td>
      <td id="T_d17b9_row4_col5" class="data row4 col5" >48</td>
      <td id="T_d17b9_row4_col6" class="data row4 col6" >0</td>
      <td id="T_d17b9_row4_col7" class="data row4 col7" >0</td>
      <td id="T_d17b9_row4_col8" class="data row4 col8" >0</td>
      <td id="T_d17b9_row4_col9" class="data row4 col9" >0</td>
      <td id="T_d17b9_row4_col10" class="data row4 col10" >0</td>
      <td id="T_d17b9_row4_col11" class="data row4 col11" >192</td>
      <td id="T_d17b9_row4_col12" class="data row4 col12" >253</td>
      <td id="T_d17b9_row4_col13" class="data row4 col13" >253</td>
      <td id="T_d17b9_row4_col14" class="data row4 col14" >156</td>
      <td id="T_d17b9_row4_col15" class="data row4 col15" >0</td>
      <td id="T_d17b9_row4_col16" class="data row4 col16" >0</td>
      <td id="T_d17b9_row4_col17" class="data row4 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row5" class="row_heading level0 row5" >5</th>
      <td id="T_d17b9_row5_col0" class="data row5 col0" >0</td>
      <td id="T_d17b9_row5_col1" class="data row5 col1" >3</td>
      <td id="T_d17b9_row5_col2" class="data row5 col2" >20</td>
      <td id="T_d17b9_row5_col3" class="data row5 col3" >20</td>
      <td id="T_d17b9_row5_col4" class="data row5 col4" >15</td>
      <td id="T_d17b9_row5_col5" class="data row5 col5" >0</td>
      <td id="T_d17b9_row5_col6" class="data row5 col6" >0</td>
      <td id="T_d17b9_row5_col7" class="data row5 col7" >0</td>
      <td id="T_d17b9_row5_col8" class="data row5 col8" >0</td>
      <td id="T_d17b9_row5_col9" class="data row5 col9" >0</td>
      <td id="T_d17b9_row5_col10" class="data row5 col10" >43</td>
      <td id="T_d17b9_row5_col11" class="data row5 col11" >224</td>
      <td id="T_d17b9_row5_col12" class="data row5 col12" >253</td>
      <td id="T_d17b9_row5_col13" class="data row5 col13" >245</td>
      <td id="T_d17b9_row5_col14" class="data row5 col14" >74</td>
      <td id="T_d17b9_row5_col15" class="data row5 col15" >0</td>
      <td id="T_d17b9_row5_col16" class="data row5 col16" >0</td>
      <td id="T_d17b9_row5_col17" class="data row5 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row6" class="row_heading level0 row6" >6</th>
      <td id="T_d17b9_row6_col0" class="data row6 col0" >0</td>
      <td id="T_d17b9_row6_col1" class="data row6 col1" >0</td>
      <td id="T_d17b9_row6_col2" class="data row6 col2" >0</td>
      <td id="T_d17b9_row6_col3" class="data row6 col3" >0</td>
      <td id="T_d17b9_row6_col4" class="data row6 col4" >0</td>
      <td id="T_d17b9_row6_col5" class="data row6 col5" >0</td>
      <td id="T_d17b9_row6_col6" class="data row6 col6" >0</td>
      <td id="T_d17b9_row6_col7" class="data row6 col7" >0</td>
      <td id="T_d17b9_row6_col8" class="data row6 col8" >0</td>
      <td id="T_d17b9_row6_col9" class="data row6 col9" >0</td>
      <td id="T_d17b9_row6_col10" class="data row6 col10" >249</td>
      <td id="T_d17b9_row6_col11" class="data row6 col11" >253</td>
      <td id="T_d17b9_row6_col12" class="data row6 col12" >245</td>
      <td id="T_d17b9_row6_col13" class="data row6 col13" >126</td>
      <td id="T_d17b9_row6_col14" class="data row6 col14" >0</td>
      <td id="T_d17b9_row6_col15" class="data row6 col15" >0</td>
      <td id="T_d17b9_row6_col16" class="data row6 col16" >0</td>
      <td id="T_d17b9_row6_col17" class="data row6 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row7" class="row_heading level0 row7" >7</th>
      <td id="T_d17b9_row7_col0" class="data row7 col0" >0</td>
      <td id="T_d17b9_row7_col1" class="data row7 col1" >0</td>
      <td id="T_d17b9_row7_col2" class="data row7 col2" >0</td>
      <td id="T_d17b9_row7_col3" class="data row7 col3" >0</td>
      <td id="T_d17b9_row7_col4" class="data row7 col4" >0</td>
      <td id="T_d17b9_row7_col5" class="data row7 col5" >0</td>
      <td id="T_d17b9_row7_col6" class="data row7 col6" >0</td>
      <td id="T_d17b9_row7_col7" class="data row7 col7" >14</td>
      <td id="T_d17b9_row7_col8" class="data row7 col8" >101</td>
      <td id="T_d17b9_row7_col9" class="data row7 col9" >223</td>
      <td id="T_d17b9_row7_col10" class="data row7 col10" >253</td>
      <td id="T_d17b9_row7_col11" class="data row7 col11" >248</td>
      <td id="T_d17b9_row7_col12" class="data row7 col12" >124</td>
      <td id="T_d17b9_row7_col13" class="data row7 col13" >0</td>
      <td id="T_d17b9_row7_col14" class="data row7 col14" >0</td>
      <td id="T_d17b9_row7_col15" class="data row7 col15" >0</td>
      <td id="T_d17b9_row7_col16" class="data row7 col16" >0</td>
      <td id="T_d17b9_row7_col17" class="data row7 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row8" class="row_heading level0 row8" >8</th>
      <td id="T_d17b9_row8_col0" class="data row8 col0" >0</td>
      <td id="T_d17b9_row8_col1" class="data row8 col1" >0</td>
      <td id="T_d17b9_row8_col2" class="data row8 col2" >0</td>
      <td id="T_d17b9_row8_col3" class="data row8 col3" >0</td>
      <td id="T_d17b9_row8_col4" class="data row8 col4" >0</td>
      <td id="T_d17b9_row8_col5" class="data row8 col5" >11</td>
      <td id="T_d17b9_row8_col6" class="data row8 col6" >166</td>
      <td id="T_d17b9_row8_col7" class="data row8 col7" >239</td>
      <td id="T_d17b9_row8_col8" class="data row8 col8" >253</td>
      <td id="T_d17b9_row8_col9" class="data row8 col9" >253</td>
      <td id="T_d17b9_row8_col10" class="data row8 col10" >253</td>
      <td id="T_d17b9_row8_col11" class="data row8 col11" >187</td>
      <td id="T_d17b9_row8_col12" class="data row8 col12" >30</td>
      <td id="T_d17b9_row8_col13" class="data row8 col13" >0</td>
      <td id="T_d17b9_row8_col14" class="data row8 col14" >0</td>
      <td id="T_d17b9_row8_col15" class="data row8 col15" >0</td>
      <td id="T_d17b9_row8_col16" class="data row8 col16" >0</td>
      <td id="T_d17b9_row8_col17" class="data row8 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row9" class="row_heading level0 row9" >9</th>
      <td id="T_d17b9_row9_col0" class="data row9 col0" >0</td>
      <td id="T_d17b9_row9_col1" class="data row9 col1" >0</td>
      <td id="T_d17b9_row9_col2" class="data row9 col2" >0</td>
      <td id="T_d17b9_row9_col3" class="data row9 col3" >0</td>
      <td id="T_d17b9_row9_col4" class="data row9 col4" >0</td>
      <td id="T_d17b9_row9_col5" class="data row9 col5" >16</td>
      <td id="T_d17b9_row9_col6" class="data row9 col6" >248</td>
      <td id="T_d17b9_row9_col7" class="data row9 col7" >250</td>
      <td id="T_d17b9_row9_col8" class="data row9 col8" >253</td>
      <td id="T_d17b9_row9_col9" class="data row9 col9" >253</td>
      <td id="T_d17b9_row9_col10" class="data row9 col10" >253</td>
      <td id="T_d17b9_row9_col11" class="data row9 col11" >253</td>
      <td id="T_d17b9_row9_col12" class="data row9 col12" >232</td>
      <td id="T_d17b9_row9_col13" class="data row9 col13" >213</td>
      <td id="T_d17b9_row9_col14" class="data row9 col14" >111</td>
      <td id="T_d17b9_row9_col15" class="data row9 col15" >2</td>
      <td id="T_d17b9_row9_col16" class="data row9 col16" >0</td>
      <td id="T_d17b9_row9_col17" class="data row9 col17" >0</td>
    </tr>
    <tr>
      <th id="T_d17b9_level0_row10" class="row_heading level0 row10" >10</th>
      <td id="T_d17b9_row10_col0" class="data row10 col0" >0</td>
      <td id="T_d17b9_row10_col1" class="data row10 col1" >0</td>
      <td id="T_d17b9_row10_col2" class="data row10 col2" >0</td>
      <td id="T_d17b9_row10_col3" class="data row10 col3" >0</td>
      <td id="T_d17b9_row10_col4" class="data row10 col4" >0</td>
      <td id="T_d17b9_row10_col5" class="data row10 col5" >0</td>
      <td id="T_d17b9_row10_col6" class="data row10 col6" >0</td>
      <td id="T_d17b9_row10_col7" class="data row10 col7" >43</td>
      <td id="T_d17b9_row10_col8" class="data row10 col8" >98</td>
      <td id="T_d17b9_row10_col9" class="data row10 col9" >98</td>
      <td id="T_d17b9_row10_col10" class="data row10 col10" >208</td>
      <td id="T_d17b9_row10_col11" class="data row10 col11" >253</td>
      <td id="T_d17b9_row10_col12" class="data row10 col12" >253</td>
      <td id="T_d17b9_row10_col13" class="data row10 col13" >253</td>
      <td id="T_d17b9_row10_col14" class="data row10 col14" >253</td>
      <td id="T_d17b9_row10_col15" class="data row10 col15" >187</td>
      <td id="T_d17b9_row10_col16" class="data row10 col16" >22</td>
      <td id="T_d17b9_row10_col17" class="data row10 col17" >0</td>
    </tr>
  </tbody>
</table>




## how a computer might be able to recognize these two different digits

- i) 이미지 텐서 스택으로 겹쳐서 이상적인 이미지 만든 다음 개별 이미지와의 거리 비교
- ii) 각각의 픽셀에 대해 각 숫자별로 최적의 weight 찾아서 개별 이미지가 해당 숫자일 확률 구하기

# First trial; to get the average of pixel values for each of our two groups




```python
# 텐서는 n차원으로 쌓아 올라갈 수 있음 (scholar(0)-vector(1)-matrix(2)-array(3)-...)
seven_tensors = [tensor(Image.open(o)) for o in sevens] # 텐서로 변환해서 리스트 만듦
three_tensors = [tensor(Image.open(o)) for o in threes]
len(three_tensors),len(seven_tensors)
```




    (6131, 6265)




```python
stacked_sevens = torch.stack(seven_tensors).float()/255 # 쌓아서 스택으로 만들기, 값 정규화
stacked_threes = torch.stack(three_tensors).float()/255
stacked_threes.shape
```




    torch.Size([6131, 28, 28])




```python
len(stacked_threes.shape)
```




    3



- rank: number of dimension, axes
- shape: size of each axis of a tensor


```python
"""stacked_threes.rank"""

# 문법이 이게 아닌가봐요,,?'Tensor' object has no attribute 'rank'
# 아하 다음 줄에 바로 나오는군요,,,

stacked_threes.ndim
```




    3




```python
# 겹쳐서 3 그리기
mean3 = stacked_threes.mean(0)
show_image(mean3);
```


    
![png](/../../images/fastai_ch4/output_19_0.png)
    



```python
# 겹쳐서 7 그리기
mean7 = stacked_sevens.mean(0)
show_image(mean7);
```


    
![png](/../../images/fastai_ch4/output_20_0.png)
    



```python
a_3 = stacked_threes[1]
show_image(a_3);
```


    
![png](/../../images/fastai_ch4/output_21_0.png)
    


## How would you calculate how similar a particular image is to each of our ideal digits?

- 각각의 이미지와 위에서 만든 이미지의 각 픽셀을 비교한다 (빼서 절댓값-mean absolute difference or L1 norm, 빼서 제곱- root mean squared error (RMSE) or L2 norm 등)


```python
F.l1_loss(a_3.float(),mean3), F.mse_loss(a_3,mean3).sqrt()
```




    (tensor(0.1114), tensor(0.2021))




```python
F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()
```




    (tensor(0.1586), tensor(0.3021))



## NumPy Arrays and PyTorch Tensors
- NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.

### NumPy array
- a multidimensional table of data, with all items of the same type
- can be any type at all (but still they are smae typed data)
- with the innermost arrays potentially being different sizes("jagged array.") 
- fast computation (opt C)

### PyTorch tensor
- a multidimensional table of data, with all items of the same type
- tensor cannot use just any old type—it has to use a single basic numeric type for all components
- cannot be jagged (always a regularly shaped multidimensional rectangular structure)
- can live on the GPU
- can automatically calculate derivatives of these operations, including combinations of operations.


```python
data = [[1,2,3],[4,5,6]]
arr = array (data)
tns = tensor(data)
```


```python
arr  # numpy
```




    array([[1, 2, 3],
           [4, 5, 6]])




```python
tns  # pytorch
```




    tensor([[1, 2, 3],
            [4, 5, 6]])




```python
tns[1]
# row
# 0-index, 1 = second
```




    tensor([4, 5, 6])




```python
tns[:,1]
# rows and cloumns
```




    tensor([2, 5])




```python
tns[1,1:3]
# rows and cloumns
```




    tensor([5, 6])




```python
tns+1
#  can use the standard operators such as +, -, *, /
```




    tensor([[2, 3, 4],
            [5, 6, 7]])




```python
tns.type()
```




    'torch.LongTensor'




```python
tns*1.5
# automatically change type as needed (e.g. from int to float)
```




    tensor([[1.5000, 3.0000, 4.5000],
            [6.0000, 7.5000, 9.0000]])



## Computing Metrics Using Broadcasting

- Broadcasting: 차원이 다른 두 객체의 연산을 위해 자동으로 차원 변환하는 것 (does the whole calculation in C, "elementwise")


```python
valid_3_tens = torch.stack([tensor(Image.open(o)) 
                            for o in (path/'valid'/'3').ls()])
valid_3_tens = valid_3_tens.float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) 
                            for o in (path/'valid'/'7').ls()])
valid_7_tens = valid_7_tens.float()/255
valid_3_tens.shape,valid_7_tens.shape
```




    (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))




```python
def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))
mnist_distance(a_3, mean3)

# ! 마지막에 .mean((-1, -2)) 무슨 의미지..?
# (-1, -2) = range of axes, 텐서의 마지막에서 첫번째, 두변째 axes
# 즉, 마지막 두 축(이미지 가로세로)에 대한 평균을 받은 뒤, 나머지 하나의 축(1010개의 사진)을 인덱스 삼아서 합체
```




    tensor(0.1114)




```python
valid_3_dist = mnist_distance(valid_3_tens, mean3)
valid_3_dist, valid_3_dist.shape

# valid_3_tens, mean3 -> deiferent rank 
# -> broadcasting(automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank)
# (valid_3_tens-mean3).shape -> torch.Size([1010, 28, 28])
```




    (tensor([0.1280, 0.1623, 0.1242,  ..., 0.1508, 0.1263, 0.1260]),
     torch.Size([1010]))




```python
# 3 vs 7
def is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)

is_3(a_3), is_3(a_3).float()
```




    (tensor(True), tensor(1.))




```python
is_3(valid_3_tens)
# broadcasting
```




    tensor([ True,  True,  True,  ..., False,  True,  True])




```python
accuracy_3s =      is_3(valid_3_tens).float() .mean()
accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()

accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2

# 3들의 abs와 7들의 abs 구한 다음 평균 = 이 모델(프로그램)의 정확도
```




    (tensor(0.9168), tensor(0.9854), tensor(0.9511))



# Second trial; to turn this function into a machine learning classifier

## to turn this function into a machine learning classifier

1) Initialize the weights.

2) For each image, use these weights to predict whether it appears to be a 3 or a 7.

3) Based on these predictions, calculate how good the model is (its loss).

4) Calculate the gradient, which measures for each weight, how changing that weight would change the loss

5) Step (that is, change) all the weights based on that calculation.

6) Go back to the step 2, and repeat the process.

7) Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer).

## Stochastic Gradient Descent (SGD)
정확도를 최대화하기 위해 자동으로 weight 보정해주는 장치
concept by Issac Newton.... wow

- to "learn":  look at each individual pixel and come up with a set of weights for each one, updates the weights until they make the best results
- e.g.) the highest weights are associated with those pixels most likely to be black for a particular category(there are sets of weight values for each possible category)
- probability of being a particular number = "sum of (each pixel)*(each weight)", and the weigh t should be updated for better performance
- derivative: 미분을 통해 방향 결정 가능
- .backward(): 각 layer의 미분값 계산, loss값 역전파해서 모델에 알려주기
- c.f. forward pass: 예측값 계산


! parameter = 여러 개, loss = 한 개 -> 방향 결정할 때 각각의 파라미터를 어떻게 움직이는지 결정하는 게 어렵지 않을까? 어떤 파라미터는 올렸을 때 성능이 좋고 어떤 건 내렸을 때 성능이 좋을텐데 판단을 어떻게 한다는거지?

*"you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight"*

이렇다고는 하는데,,, 구체적으로는 모르겠다!

### terms
- initialize: initialize the parameters to random values
- Loss: the number returned by a function that will return a number that is small if the performance of the model is good
- step: moving a bit after finding the right direction
- (stepping parameter = updating parameter: w = w - gradient(w) * lr, using an  optimizer step)
- stop: stop learning when we meet minimal loss, or took too much time

## applying (a sample, not the MNIST)

### Step 1: Initialize the parameters


```python
time = torch.arange(0,20).float(); time
speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1

def f(t, params):
    a,b,c = params
    return a*(t**2) + (b*t) + c

def mse(preds, targets): return ((preds-targets)**2).mean()

```


```python
params = torch.randn(3).requires_grad_()
orig_params = params.clone()
```


```python
params
```




    tensor([-0.2384, -0.3417,  0.3040], requires_grad=True)



### Step 2: Calculate the predictions


```python
preds = f(time, params)
```


```python
# Visualization
def show_preds(preds, ax=None):
    if ax is None: ax=plt.subplots()[1]
    ax.scatter(time, speed)
    ax.scatter(time, to_np(preds), color='red')
    ax.set_ylim(-300,100)
```


```python
show_preds(preds)
```


    
![png](/../../images/fastai_ch4/output_54_0.png)
    


### Step 3: Calculate the loss


```python
loss = mse(preds, speed)
loss

```




    tensor(5041.2041, grad_fn=<MeanBackward0>)



### Step 4: Calculate the gradients


```python
loss.backward()
params.grad
```




    tensor([-22265.3652,  -1429.9702,   -116.4298])




```python
params.grad * 1e-5
```




    tensor([-0.2227, -0.0143, -0.0012])




```python
params
```




    tensor([-0.2384, -0.3417,  0.3040], requires_grad=True)



### Step 5: Step the weights.


```python
lr = 1e-5
params.data -= lr * params.grad.data
# We use the magnitude of the gradient 
# (i.e., the steepness of the slope) to tell us how big a step to take
params.grad = None
```


```python
preds = f(time,params)
mse(preds, speed)
```




    tensor(1469.4380, grad_fn=<MeanBackward0>)




```python
show_preds(preds)
```


    
![png](/../../images/fastai_ch4/output_64_0.png)
    



```python
def apply_step(params, prn=True):
    preds = f(time, params)
    loss = mse(preds, speed)
    loss.backward()
    params.data -= lr * params.grad.data
    params.grad = None
    if prn: print(loss.item())
    return preds
```

### Step 6: Repeat the process


```python
for i in range(10): apply_step(params)
```

    1469.43798828125
    793.5477905273438
    665.6459350585938
    641.4403076171875
    636.8569946289062
    635.9868774414062
    635.8193969726562
    635.784912109375
    635.775634765625
    635.7709350585938
    


```python
params = orig_params.detach().requires_grad_()
```


```python
_,axs = plt.subplots(1,4,figsize=(12,3))
for ax in axs: show_preds(apply_step(params, False), ax)
plt.tight_layout()
```


    
![png](/../../images/fastai_ch4/output_69_0.png)
    


### Step 7: stop

## real aplication (the MNIST)

### Step 0: preparing dataset


```python
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
# view: a PyTorch method that changes the shape of a tensor without changing its contents. 
# -1: a special parameter to view that means "make this axis as big as necessary to fit all the data"
```


```python
# a label for each image (0 = 3)
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
# 각각의 id에 할당할 필요 없이, train_x 만들 때 3 다음 7 순서로 얌전하게 concat했으면
# 그냥 len(threes), len(sevens) 이용해서 개수만 맞게 1, 0 넣어주면 됨
train_x.shape,train_y.shape
```




    (torch.Size([12396, 784]), torch.Size([12396, 1]))




```python
# to return a tuple of (x,y) when indexed, dataset pytorch needs it
# zip+list = easy to make tuple
dset = list(zip(train_x,train_y))
x,y = dset[0]
# 0번을 인덱싱한 이유는 아래에서 각각의 x y 모양 확인하려고
x.shape,y
# x만 shape인 이유는 형태가 다르니까
```




    (torch.Size([784]), tensor([1]))




```python
# valid도 똑같이 데이터셋 준비
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)
valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
```

### Step 1: Initialize the parameters


```python
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
# 입력값 사이즈를 넣으면 사이즈 중에 random number에다가 std(내가 정함)곱한 값을 역전파
# initializing 함수
weights = init_params((28*28,1))
bias = init_params(1)
# 이걸로 bias도 만듦
```

### Step 2: Calculate the predictions


```python
# sample
(train_x[0]*weights.T).sum() + bias
```




    tensor([6.7826], grad_fn=<AddBackward0>)




```python
# matrix multiplication (represented with the @ operator)
def linear1(xb): return xb@weights + bias 
#  = batch@weights + bias
# linear1 + init_params = nn.Linear

preds = linear1(train_x)
preds
```




    tensor([[ 6.7826],
            [11.8972],
            [ 8.0469],
            ...,
            [-9.7835],
            [ 0.2792],
            [-6.2459]], grad_fn=<AddBackward0>)




```python
# checking the accuracy
corrects = (preds>0.0).float() == train_y
corrects
```




    tensor([[ True],
            [ True],
            [ True],
            ...,
            [ True],
            [False],
            [ True]])



not accurate actally (마지막 부분에 True False 섞여있음)


```python
corrects.float().mean().item()
```




    0.7631493806838989



### Step 3: Calculate the loss

problem: accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa (classification problem, not regression)

-> This then gives gradients that are 0 or infinite

-> a 3 the score(possibility) is a little higher, or if the correct answer is a 7 the score is a little lower

->  = 1(label of 3) - prds (of values between 0 and 1)


```python
def mnist_loss(predictions, targets):
    return torch.where(targets==1, 1-predictions, predictions).mean()

# torch.where(a,b,c): [b[i] if a[i] else c[i] for i in range(len(a))]
# lower return = more accurate
# ex. 
"""trgts  = tensor([1,0,1])
prds   = tensor([0.9, 0.4, 0.2])
torch.where(trgts==1, 1-prds, prds) """
# --->
"""tensor([0.1000, 0.4000, 0.8000])"""
```


```python
def sigmoid(x): return 1/(1+torch.exp(-x))
# to make the vlaues between 0 and 1
```


```python
# 시그모이드 적용
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
```

### Step 4: Calculate the gradients



```python
with torch.no_grad(): weights[0] *= 1.0001

preds = linear1(train_x)
((preds>0.0).float() == train_y).float().mean().item()
```




    0.7631493806838989




```python
# picking good size of batch
# shuffling the order of data after each epoch, before making a batch
# = dataloader

# ex.
"""coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)"""
# --->
"""[tensor([ 3, 12,  8, 10,  2]),
 tensor([ 9,  4,  7, 14,  5]),
 tensor([ 1, 13,  0,  6, 11])]"""
```




    [tensor([ 3, 12,  8, 10,  2]),
     tensor([ 9,  4,  7, 14,  5]),
     tensor([ 1, 13,  0,  6, 11])]




```python
# dataset: which would be devided by the dataloader, containing x and y

# ex. 
"""ds = L(enumerate(string.ascii_lowercase))
ds"""
# ---> 
"""(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd')...]"""
```




    (#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]




```python
# so we put dataset into a dataloader

# ex. 
"""dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)"""
# ---> 
"""[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
 (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
 (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
 (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
 (tensor([2, 4]), ('c', 'e'))]"""
```




    [(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
     (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
     (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
     (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
     (tensor([2, 4]), ('c', 'e'))]




```python
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl)
xb.shape,yb.shape
```




    (torch.Size([256, 784]), torch.Size([256, 1]))




```python
valid_dl = DataLoader(valid_dset, batch_size=256)
```


```python
def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
    
# preds = linear1(train_x)
# loss = mnist_loss(preds, train_y)
#loss.backward()
```


```python
# test
batch = train_x[:4]
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
```




    (tensor(-0.0006), tensor([-0.0034]))




```python
# test2 (to see whether the gradient has changed) -> changed
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
```




    (tensor(-0.0012), tensor([-0.0067]))




```python
# the reason of the cjanged grad 
# =  loss.backward actually adds the gradients of loss to any gradients that are currently stored
"""weights.grad.zero_()
bias.grad.zero_();"""
```

### Step 5: Step the weights.


```python
def train_epoch(model, lr, params):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
```


```python
# test
batch = train_x[:4]
preds = linear1(batch)
(preds>0.0).float() == train_y[:4]
```




    tensor([[True],
            [True],
            [True],
            [True]])




```python
def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds>0.5) == yb 
    # 시그모이드 거친 xb(모델 돌린 결과값)가 0.5가 넘는 것이 yb와 같으면
    # correct 
    # ! correct가 리스트도 아닌데 어떻게 누적이 되는거지?
    return correct.float().mean()
```


```python
# test
batch_accuracy(linear1(batch), train_y[:4])
```




    tensor(1.)




```python
def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    # valid_dl 속 xb, yb에 대해 batch accuracy를 돌린 결과를 
    # accs 리스트에 저장
    return round(torch.stack(accs).mean().item(), 4)
```


```python
# test
validate_epoch(linear1)
```




    0.8021




```python
# application (mnist)

lr = 1.  # 학습률 너무 크게 잡았다
params = weights,bias
train_epoch(linear1, lr, params)
validate_epoch(linear1)
```




    0.835



### Step 6: Repeat the process


```python
for i in range(20):
    train_epoch(linear1, lr, params)
    print(validate_epoch(linear1), end=' ')
```

    0.9292 0.9541 0.9619 0.9663 0.9692 0.9702 0.9721 0.9731 0.9736 0.9755 0.976 0.976 0.976 0.9765 0.977 0.977 0.9765 0.9765 0.9765 0.9765 

### extra stetp: optimizer


```python
# module used ()
linear_model = nn.Linear(28*28,1)
```


```python
# 옴티마이저 만들기 위한 정보 얻기
w,b = linear_model.parameters()
w.shape,b.shape
```




    (torch.Size([1, 784]), torch.Size([1]))




```python
class BasicOptim:
    def __init__(self,params,lr): self.params,self.lr = list(params),lr

    def step(self, *args, **kwargs):
        for p in self.params: p.data -= p.grad.data * self.lr

    def zero_grad(self, *args, **kwargs):
        for p in self.params: p.grad = None
```


```python
opt = BasicOptim(linear_model.parameters(), lr)
```


```python
def train_epoch(model):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        opt.step()        # 파라미터 update
        opt.zero_grad()   # grad 초기화
```


```python
def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    return round(torch.stack(accs).mean().item(), 4)
```


```python
validate_epoch(linear_model)
```

    0.8325
    


```python
def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end=' ')
```


```python
train_model(linear_model, 20)
```

    0.8467 0.9141 0.9351 0.9477 0.9555 0.9619 0.9658 0.9673 0.9697 0.9726 0.9731 0.9751 0.9761 0.9765 0.9775 0.978 0.978 0.979 0.979 0.979 


```python
# SGD 모듈 이용

linear_model = nn.Linear(28*28,1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
```

    0.4932 0.832 0.8364 0.9106 0.9346 0.9468 0.956 0.9624 0.9658 0.9678 0.9697 0.9721 0.9741 0.9751 0.9756 0.9765 0.9775 0.978 0.978 0.979 


```python
# Learner.fit 이용
# dataloader에 데이터 넣어서 변수에 저장해두기

dls = DataLoaders(dl, valid_dl)

learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,
                loss_func=mnist_loss, metrics=batch_accuracy)

learn.fit(10, lr=lr)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.636342</td>
      <td>0.503504</td>
      <td>0.495584</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.533367</td>
      <td>0.185639</td>
      <td>0.845927</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.195557</td>
      <td>0.181157</td>
      <td>0.837095</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.085474</td>
      <td>0.105941</td>
      <td>0.912169</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.044914</td>
      <td>0.077488</td>
      <td>0.933759</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.029076</td>
      <td>0.062201</td>
      <td>0.946516</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.022569</td>
      <td>0.052639</td>
      <td>0.955839</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.019684</td>
      <td>0.046253</td>
      <td>0.962709</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.018230</td>
      <td>0.041760</td>
      <td>0.965653</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.017364</td>
      <td>0.038452</td>
      <td>0.967125</td>
      <td>00:01</td>
    </tr>
  </tbody>
</table>


# Second trial + ; adding activation function (non-linear) at the linear classifier

## neural network


```python
def simple_net(xb): 
    res = xb@w1 + b1  # linear, layer
    res = res.max(tensor(0.0))  #  = relu, nonlinear, actfunc
    res = res@w2 + b2  # linear, layer
    return res
```


```python
w1 = init_params((28*28,30)) 
# std = 30 
# 30가지의 output 
# (def init_params(size, std): return (torch.randn(size)*std).requires_grad_())
b1 = init_params(30)
# 30가지의 bias
w2 = init_params((30,1))
# 위에서 받은 30개의 아웃풋이 max 거친 후 (= 여전히 30개)
b2 = init_params(1)
# 1가지의 bias
```

만약 선형함수만 계속 쌓여있다면 그냥 선형함수임. 

(여러 개를 곱한 뒤 더하는 것을 반복( = a(ax+b)+b)하는 것은 그냥 여러번 곱하고 한 번 더하는 것(=a^2x+(ab+b))과 같음. )

-> actfunc 통해서 선형함수를 분리시켜줌


```python
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)

# nn.Sequential: 적힌 각각의 레잉/함수를 차례대로 불러오는 모듈
# 모듈이기 때문에 파라미터 리스트를 return 해줄것임
```


```python
learn = Learner(dls, simple_net, opt_func=SGD,
                loss_func=mnist_loss, metrics=batch_accuracy)

learn.fit(40, 0.1)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.310149</td>
      <td>0.418824</td>
      <td>0.504416</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.145563</td>
      <td>0.231791</td>
      <td>0.800294</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.080435</td>
      <td>0.115442</td>
      <td>0.916094</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.052789</td>
      <td>0.077257</td>
      <td>0.943572</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.039994</td>
      <td>0.060009</td>
      <td>0.956820</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.033472</td>
      <td>0.050419</td>
      <td>0.965162</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.029731</td>
      <td>0.044423</td>
      <td>0.966143</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.027306</td>
      <td>0.040337</td>
      <td>0.966634</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.025561</td>
      <td>0.037372</td>
      <td>0.969578</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.024213</td>
      <td>0.035110</td>
      <td>0.972522</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.023122</td>
      <td>0.033317</td>
      <td>0.973503</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.022212</td>
      <td>0.031853</td>
      <td>0.973994</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.021436</td>
      <td>0.030626</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.020764</td>
      <td>0.029577</td>
      <td>0.975466</td>
      <td>00:01</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.020174</td>
      <td>0.028663</td>
      <td>0.975957</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.019651</td>
      <td>0.027859</td>
      <td>0.976448</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.019182</td>
      <td>0.027143</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.018759</td>
      <td>0.026501</td>
      <td>0.978901</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.018374</td>
      <td>0.025924</td>
      <td>0.978901</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.018022</td>
      <td>0.025400</td>
      <td>0.979392</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.017698</td>
      <td>0.024923</td>
      <td>0.979882</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.017398</td>
      <td>0.024487</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.017119</td>
      <td>0.024087</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.016859</td>
      <td>0.023718</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.016615</td>
      <td>0.023378</td>
      <td>0.980864</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.016386</td>
      <td>0.023063</td>
      <td>0.980864</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.016170</td>
      <td>0.022771</td>
      <td>0.980864</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.015966</td>
      <td>0.022499</td>
      <td>0.981845</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.015773</td>
      <td>0.022246</td>
      <td>0.981845</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.015589</td>
      <td>0.022010</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.015415</td>
      <td>0.021789</td>
      <td>0.982826</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.015248</td>
      <td>0.021582</td>
      <td>0.983317</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.015089</td>
      <td>0.021388</td>
      <td>0.983317</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.014937</td>
      <td>0.021206</td>
      <td>0.983317</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.014791</td>
      <td>0.021035</td>
      <td>0.983317</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.014651</td>
      <td>0.020873</td>
      <td>0.983808</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.014517</td>
      <td>0.020720</td>
      <td>0.983808</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.014388</td>
      <td>0.020575</td>
      <td>0.983808</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.014263</td>
      <td>0.020438</td>
      <td>0.983808</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.014142</td>
      <td>0.020308</td>
      <td>0.983808</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>


deeper model -> not need to use many params -> smaller matric, beeter results

# trial ends

## Questionnaire

1) How is a grayscale image represented on a computer? How about a color image?
- gray: with pixels of 0 to 255, colcored: with 3 channels (R, G, B)

2) How are the files and folders in the MNIST_SAMPLE dataset structured? Why?
- train/valid -> 3/7

3) Explain how the "pixel similarity" approach to classifying digits works.
- find out the ideal 3, 7 by meaning the pixels
- subtract each pixel of the data from the idal one
- square/absolute

4) What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.
- odd_nums = [tensor(2*i+1) for i in range(10)]

5) What is a "rank-3 tensor"?
- tensor with 3 axes

6) What is the difference between tensor rank and shape? How do you get the rank from the shape?
- rank: number of axes
- shape: length of each axes
- len(tensor_name.shape)

7) What are RMSE and L1 norm?
- RMSE:  root mean squared error
- L1: mean absolute difference

8) How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?
- by using broadcasting

9) Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.
- a = tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])
- a*2
- ! bottom-right... 4..? 

10) What is broadcasting?
- autonmatically downgrading the dimension of a tensor to do some calculation

11) Are metrics generally calculated using the training set, or the validation set? Why?
- valid set
- It's sth to test the accuracy of prediction

12) What is SGD?
- making little steps until meeting the lowest loss (2차 함수)

13) Why does SGD use mini-batches?
- to train the model quickly and accurately
- to load enough amount of work to GPU 

14) What are the seven steps in SGD for machine learning?
- it's up there..

15) How do we initialize the weights in a model?
- randomly

16) What is "loss"?
- a number shows how well the model is doing (lower = better)

17) Why can't we always use a high learning rate?
- may can't meet the lowest loss forever, "bounce" around

18) What is a "gradient"?
- a derivation/inclination of a loss function

19) Do you need to know how to calculate gradients yourself?
- nope

20) Why can't we use accuracy as a loss function?
- most of the time our gradients will actually be 0, and the model will not be able to learn from that number.

21) Draw the sigmoid function. What is special about its shape?
- can make any number into range of 0 to 1, very smoothly

22) What is the difference between a loss function and a metric?
- metric: a tool of estimating the accuracy of the model
- loss function: a calculation to find out the loss

23) What is the function to calculate new weights using a learning rate?
- calculus to calculate the gradients, "with torch.no_grad()"

24) What does the DataLoader class do?
- do the shuffling and make mini-batch collation

25) Write pseudocode showing the basic steps taken in each epoch for SGD.
- train with a batch
- predict
- calc loss
- loss.backward
- update params
- train ...

26) Create a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?
- a = tensor([1,2,3,4])
- b = transor(abcd)     # ! 여기서 ''붙이거나 []에 넣어도 자료형 오류,,
- set = list(zip(a, b)

27) What does view do in PyTorch?
- changes the shape of a tensor without changing its contents

28) What are the "bias" parameters in a neural network? Why do we need them?
- to make it more flexible 

29) What does the @ operator do in Python?
- matrix multiplication

30) What does the backward method do?
- give the loss to update the params

31) Why do we have to zero the gradients?
- to clear all the gradients previously saved
- ! 근데 정확히 왜 초기화시켜야 하는지는 잘 모르겠다

32) What information do we have to pass to Learner?
- data devied into batches
- function (with data size and numbers)
- optimizer function
- loss function
- metrics

33) Show Python or pseudocode for the basic steps of a training loop.
- def train_epoch(model, lr, params):
    for xb,yb in dl:                 # dl은 calc_grad 정의할 때 나옴
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()

34) What is "ReLU"? Draw a plot of it for values from -2 to +2.
- if x <0 return 0
- if x >=0 return x

35) What is an "activation function"?
- a func that seperates linear funcs

36) What's the difference between F.relu and nn.ReLU?
- moduled or not

37) The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?
- thing like 18-layer model


## Further Research

1) Create your own implementation of Learner from scratch, based on the training loop shown in this chapter.

2) Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way.
- 마지막에 나온 결과가 다 된거 아닌가요,,??
