---
layout: post
title: "fastai_ch2"
---




```python
!pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
```


```python
from fastbook import *
from fastai.vision.widgets import *
```

## From Model to Production

### Starting Your Project

- The thing is to start quickly
- don't try to find/label perfect dataset
- no perfect GUI
- Instead, complete every step as well as you can in a reasonable amount of time, all the way to the end.
- in the early iterations you take some shortcuts: similar problems , have some good examples (something from a similar domain, or measured in a different way, tackling a slightly different problem)

## The State of Deep Learning

### Computer vision
- lots of good results
- out-of-domain data: should check the data you used for model training
- labeling is expensive: data augmentation (+ few/zero shot)
- there are some domains that can be converted into image (things like sound)

### Text (nlp)
- classfying 
- generating context- appropriate text (not that correct yet: dangerous so the user should concern about that, things like summarize long documents)
- translate texts

### Combining text and images
- a deep learning model can be trained on input images with output captions written in English, and can learn to generate appropriate captions
- ! 이 반대가 zero shot?

### Tabular data
- Deep learning does greatly increase the variety of columns that you can include
    - columns containing natural language (book titles, reviews, etc.
    - and high-cardinality categorical columns (i.e., something that contains a large number of discrete choices, such as zip code or product ID)
- deep learning models generally take longer to train than random forests or gradient boosting machines
    - GPU acceleration

### Recommendation systems
- form of collaborative filtering to fill in the matrix
- they only tell you what products a particular user might like, rather than what recommendations would be helpful for a user.
- a user might like, but not be at all helpful
    - if the user is already familiar with the products
    - if they are simply different packagings of products
<br/>
---------------
- Objective of a recommendation engine is to drive additional sales by surprising and delighting the customer
- The lever is the ranking of the recommendations. 
- New data must be collected to generate recommendations that will cause new sales: require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers
- Finally, you could build two models for purchase probabilities, conditional on seeing or not seeing a recommendation. (추천 안했는데 사는 경우나 이미 샀는데 추천하는 경우 등)

### Other data types
- protein chains: look a lot like natural language documents (long sequence of tokens) -> protein analysis: NLP deep learning
- sound (= spectrograms): images

## The Drivetrain Approach
- accurate but no use
- inaccurate but useful

-> Drivetrain Approach

1. find out objective idea ("What is the user’s main objective?")
<br/>
2. think about what actions you can take (to consider what objective levers you can pull)
<br/>
3. think about what data you have (what data we already have and what additional data we will need to collect, determine the models we can build. )
<br/>
4. build a model that you can use to determine the best actions (build increasingly sophisticated products, they need a systematic design approach)

## Gathering Data


```python
key = os.environ.get('AZURE_SEARCH_KEY', 'XXX')
```


```python
search_images_bing  
```


```python
results = search_images_bing(key, 'grizzly bear')
ims = results.attrgot('contentUrl')
len(ims)
```


```python
ims = ['http://3.bp.blogspot.com/-S1scRCkI3vY/UHzV2kucsPI/AAAAAAAAA-k/YQ5UzHEm9Ss/s1600/Grizzly%2BBear%2BWildlife.jpg']
```


```python
dest = 'images/grizzly.jpg'
download_url(ims[0], dest)
```


```python
im = Image.open(dest)
im.to_thumb(128,128)
```


```python
bear_types = 'grizzly','black','teddy'
path = Path('bears')
```


```python
if not path.exists():
    path.mkdir()
    for o in bear_types:
        dest = (path/o)
        dest.mkdir(exist_ok=True)
        results = search_images_bing(key, f'{o} bear')
        download_images(dest, urls=results.attrgot('contentUrl'))
```


```python
fns = get_image_files(path)
fns
# 불러오기 실패한 데이터들
```


```python
failed = verify_images(fns)
failed
# 불러오기 실패한 데이터들 확인
```

- To remove all the failed images, you can use unlink on each of them
- return a collection, "verify_images" returns an object of type L, which includes the map method


```python
failed.map(Path.unlink);
```

#### <Sidebar: Getting Help in Jupyter Notebooks>


```python
??verify_images
```

- This tells us what argument the function accepts (fns), then shows us the source code and the file it comes from. 
- Looking at that source code, we can see it applies the function verify_image in parallel and only keeps the image files for which the result of that function is False, which is consistent with the doc string:

- Tab: if you don't remember the exact spelling of a function or argument name, autocompletion suggestions
- Shift and Tab simultaneously: will display a window with the signature of the function and a short description
- In a cell, typing "?func_name": open a window with the signature of the function and a short description.
- In a cell, typing "??func_name": will open a window with the signature of the function, a short description, and the source code.
- (fastai library) executing "doc(func_name)":  open a window with the signature of the function, a short description and links to the source code on GitHub and the full documentation of the function in the library docs.
- "%debug" in the next cell and execute: open the Python debugger, which will let you inspect the content of every variable.

- the world is full of biased data
- so be ure to think carefully about the types of data that you might expect to see in practice in your application, 
- check carefully to ensure that all these types are reflected in your model's source data.
    - tried to find out good skin, but gets young white women picture only, by Bing

## From Data to DataLoaders

- DataLoaders
    - a thin class that just stores whatever
    - DataLoader objects you pass to it, 
    - DataLoader makes objects available as train and valid


```python
"""class DataLoaders(GetAttr):
    def __init__(self, *loaders): self.loaders = loaders
    def __getitem__(self, i): return self.loaders[i]
    train,valid = add_props(lambda i,self: self[i])"""
```

- datablock api로 dataloader을 커스터마이즈

- dataloader needs
    - What kinds of data we are working with
    - How to get the list of items: get_image_files로 path 가져오기
    - How to label these items: get_y=parent_label (폴더별로 저장했기 때문에)
    - How to create the validation set: splitter=RandomSplitter(valid_pct=0.2, seed=42


```python
bears = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=Resize(128)) # 모두 같은 사이즈로 맞춰주기
```

- 커스터마이즈 한 다음
- 한 변수에 설정한 파라미터들을 저장시켜두고
- .dataloaders로 데이터배치를 gpu로 불러오고
- 데이터가 있는 곳의 path를 넣어준다


```python
dls = bears.dataloaders(path)
```


```python
dls.valid.show_batch(max_n=4, nrows=1)
```


```python
bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=4, nrows=1)
```


```python
bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=4, nrows=1)
```

- 자르거나, 덧대거나, sqeeze 다 좋지 않은 결과
- -> 사진을 잘라서 한 번만 쓰지 않고 부분을 여러 번 나눠서 잘라서 사용
- -> resize (X), randomsizedcrop(O) 같은 이미지가 여러번 사용되게 하려면 unique = True

### Data Augmentation

- Data augmentation refers to creating random variations of our input data
- resize -> aug: 이거 presizing으로 datablock에서 처리하면 더 좋다
- ! 질문: resize 말고 randomsizedcrop이랑 aug랑 결합해서 쓰면 안되나
- 바로 다음 줄에 된다고 나옴,,, 머쓱

### Training Your Model, and Using It to Clean Your Data


```python
bears = bears.new(
    item_tfms=RandomResizedCrop(224, min_scale=0.5),
    batch_tfms=aug_transforms())
dls = bears.dataloaders(path)
```


```python
learn = vision_learner(dls, resnet18, metrics=error_rate)
learn.fine_tune(2)
```

- confusion matrix 이용하여 가장 많이 헷갈리는 부분 알아내기
- -> dataset 문제인지, model 문제인지 알아내기
- -> loss 높은 이미지 보기
- -> !접근 자체는 좋지만 교재에서 1번째 이미지 라벨 잘못되었다고 하는데 내 생각에는 맞게 되어있는듯..??

- -> 어쨌든 이런 경우에는 ImageClassifierCleaner 이용해서 로스가 큰 순으로 보고 다시 카테고리 정할 수 있음

- -> 이미지 자체가 부적절한 경우는 delete (여기서 드롭다운 선택은 실제 삭제가 아니라 delete 목록에 해당 이미지 넣어주기, 실제 삭제하거나 변경하려면 해당 코드를 다시 돌려야 함)
    > for idx in cleaner.delete(): <br/>
    >   cleaner.fns[idx].unlink()
    <br/>
    > for idx,cat in cleaner.change(): <br/>
    >   shutil.move(str(cleaner.fns[idx]), path/cat)
    


```python
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix()
```


```python
interp.plot_top_losses(5, nrows=1)
```


```python
cleaner = ImageClassifierCleaner(learn)
cleaner
```

## Turning Your Model into an Online Application

### Using the Model for Inference

- save both parameters and architecture (even dls) with "export"

- model for prediction = inference
- load_learner 이용해서 만든 모델 불러오기
- 불러온 모델.predict 해서 예측하기


```python
learn.export()
```


```python
path = Path()
path.ls(file_exts='.pkl')
```


```python
learn_inf = load_learner(path/'export.pkl')
```


```python
learn_inf.predict('images/grizzly.jpg')
```


```python
# 모델에 들어있는 타겟군 단어

learn_inf.dls.vocab
```

### Creating a Notebook App from the Model

- IPython widgets (ipywidgets): GUI components, JAvaScript + Python functionality in a web browser
- Voilà: system for making apps available to end users


```python
btn_upload = widgets.FileUpload()
btn_upload
```


```python
# classify 버튼 생성

btn_run = widgets.Button(description='Classify')
btn_run
```


```python
# classify 버튼을 누르면 일어날 일

def on_click_classify(change):
    img = PILImage.create(btn_upload.data[-1]) #선택한 사진 보이게 하기
    out_pl.clear_output() #선택한 사진 보이게 하기
    with out_pl: display(img.to_thumb(128,128)) #선택한 사진 보이게 하기
    pred,pred_idx,probs = learn_inf.predict(img) # 예측하기
    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}' # 결과 보이게 하기

btn_run.on_click(on_click_classify)
```


```python
btn_upload = widgets.FileUpload() # GUI 위해 vertical box에 넣기
```


```python
VBox([widgets.Label('Select your bear!'), 
      btn_upload, btn_run, out_pl, lbl_pred])
```

### Turning Your Notebook into a Real App


```python
!pip install voila
!jupyter serverextension enable --sys-prefix voila
```

### Deploying your app

- gpu x
    - 하나의 이미지 분류에는 필요X
    - 동시 사용자는 기다리라고 하면 됨
    - 큐 시스템 복잡
    - cpu 서버가 더 저렴
<br/>
<br/>
- binder로 퍼블리싱 (streamlit 이나 huggingface ?)
    - Add your notebook to a GitHub repository.
    - Paste the URL of that repo into Binder's URL, as shown in <>.
    - Change the File dropdown to instead select URL.
    - In the "URL to open" field, enter /voila/render/name.ipynb (replacing name with the name of for your notebook).
    - Click the clickboard button at the bottom right to copy the URL and paste it somewhere safe.
    - Click Launch.
<br/>
<br/>
- 비올라, cpu의 단점
    - 디바이스에서 바로 작동되게 하는건 가성비가 낮다
    - 모델이 불러와지는데에 걸리는 레이턴시는 있을 수 있다
    - 개인정보 문제가 있다면 방화벽같은 것을 사용해야 할지도, 각 말단 장치에서 실행하는 경우 사용자 확장은 쉬움
    - apple은 머신러닝 모델을 ios에서 효율적으로 작동시킬 수 있도록 컨버팅하는 api 제공


## How to Avoid Disaster

- managing multiple versions of models
- A/B testing
- canarying
- refreshing the data (should we just grow and grow our datasets all the time, or should we regularly remove some of the old data?)
- handling data labeling
- monitoring all this
- detecting model rot...



### ood data 문제

- Working with video data instead of images
- Handling nighttime images, which may not appear in this dataset
- Dealing with low-resolution camera images
- Ensuring results are returned fast enough to be useful in practice
- Recognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)
- 특히 인터넷 자료들은 아주 잘 작업된 것들일 수 있다 -> own data collection needed

### domain shift 문제
- 우리 모델이 다루는 데이터가 시간이 가면서 계속 바뀌는 문제


### 두 문제의 해결 방법
- 이 두 문제는 우리가 딥러닝 모델의 정확한 동작 과정을 알 수 없기 때문에 해결하기 어렵다는 문제가 있다


1. manual process
- run model in parallel
- human checks all predictions

2. limited scope deployment
- careful human supervision
- time or geography limited (특정 상황에 잘 맞게 특화시키기)

3. gradual expansion
- good reporting system needed
- consider what could go wrong


### Unforeseen Consequences and Feedback Loops

- 모델로 예측하는 행위 자체가 투입되는 데이터를 변화시키는 경우가 있을 수 있다 (범죄 예측)
- what if the predictive power was extremely high, and its ability to influence behavior was extremely significant?
- In that case, who would be most impacted?
- What would the most extreme results potentially look like?
- How would you know what was really going on?

### Get Writing!

- blogging!
