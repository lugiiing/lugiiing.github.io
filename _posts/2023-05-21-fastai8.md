---
layout: post
title: "fastai_ch8"
---


## Collaborative Filtering Deep Dive(PMF, probablistic matrix factorization)


```python
! pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
```


```python
from fastbook import *
```

- = 추천시스템, 각 데이터의 특징은 몰라도 특정 데이터(상품)의 소비자 / 관련자의 다른 연관성 (소비)를 통해 추천가능
- latent factors (사용자와 아이템이 알 수 없는 차원에 표현 → 사용자의 매핑지점과 아이템 매핑지점이 가까울수록 유사하다 )

→ 밴드추천 시스템 (좋아하는 가수/ 아이돌/ 배우 등 → 장르에맞는 밴드 : 필요한것은 현재 팬들의 다른 관심사)

- Matrix Factorization : 사용자에 대한 matrix (초기 고유번호, 각 번호가 무엇읗 나타내는지는 모름) 와, 아이템에 대한 초기 고유번호를 바탕으로 내적한 다음, 내적값(dot product)이 실제 평점과 유사해지도록 matrix을 학습시킨다

## A First Look at Data

Movie lens 데이터 이용


```python
from fastai.collab import *
from fastai.tabular.all import *
path = untar_data(URLs.ML_100k)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>





<div>
  <progress value='4931584' class='' max='4924029' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.15% [4931584/4924029 00:01&lt;00:00]
</div>



- u-datatype: tab seperated


```python
ratings = pd.read_csv(path/'u.data', delimiter='\t', header=None,
                      names=['user','movie','rating','timestamp'])
ratings.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user</th>
      <th>movie</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>242</td>
      <td>3</td>
      <td>881250949</td>
    </tr>
    <tr>
      <th>1</th>
      <td>186</td>
      <td>302</td>
      <td>3</td>
      <td>891717742</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>377</td>
      <td>1</td>
      <td>878887116</td>
    </tr>
    <tr>
      <th>3</th>
      <td>244</td>
      <td>51</td>
      <td>2</td>
      <td>880606923</td>
    </tr>
    <tr>
      <th>4</th>
      <td>166</td>
      <td>346</td>
      <td>1</td>
      <td>886397596</td>
    </tr>
  </tbody>
</table>
</div>



### 학습과정
moviextab 유저 id 당 영화 id 평점 표

- 빈 공간은 모델이 채워야할 것

1) randomly initialize latent factors of each user and movie
2) claculatimg predicts (dot product)
- ! 여기서 latent factor에 의미가 있는 것처럼 설명했는데 (e.g. 액션...) 실제로 각 latent factor에 의미가 있는것인지 설명을 위햐 비유한것인지..헷결려요ㅠㅠ
3) loss clac
4) optimize latent factor with Sgd

### creating data loader


```python
movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',
                     usecols=(0,1), names=('movie','title'), header=None)
movies.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>movie</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Toy Story (1995)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>GoldenEye (1995)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Four Rooms (1995)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Get Shorty (1995)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Copycat (1995)</td>
    </tr>
  </tbody>
</table>
</div>




```python
ratings = ratings.merge(movies)
ratings.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user</th>
      <th>movie</th>
      <th>rating</th>
      <th>timestamp</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>242</td>
      <td>3</td>
      <td>881250949</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>63</td>
      <td>242</td>
      <td>3</td>
      <td>875747190</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>226</td>
      <td>242</td>
      <td>5</td>
      <td>883888671</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>154</td>
      <td>242</td>
      <td>3</td>
      <td>879138235</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>306</td>
      <td>242</td>
      <td>5</td>
      <td>876503793</td>
      <td>Kolya (1996)</td>
    </tr>
  </tbody>
</table>
</div>




```python
dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)
dls.show_batch()
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user</th>
      <th>title</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>542</td>
      <td>My Left Foot (1989)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>422</td>
      <td>Event Horizon (1997)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>311</td>
      <td>African Queen, The (1951)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>595</td>
      <td>Face/Off (1997)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>617</td>
      <td>Evil Dead II (1987)</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>158</td>
      <td>Jurassic Park (1993)</td>
      <td>5</td>
    </tr>
    <tr>
      <th>6</th>
      <td>836</td>
      <td>Chasing Amy (1997)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>474</td>
      <td>Emma (1996)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>466</td>
      <td>Jackie Chan's First Strike (1996)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>554</td>
      <td>Scream (1996)</td>
      <td>3</td>
    </tr>
  </tbody>
</table>


- To represent collaborative filtering in PyTorch we can't just use the crosstab representation directly


```python
n_users  = len(dls.classes['user'])
n_movies = len(dls.classes['title'])
n_factors = 5

user_factors = torch.randn(n_users, n_factors)
movie_factors = torch.randn(n_movies, n_factors)
```

- To calc dot product -> lookup in an index (movie/user) -> what deep learning doesn't do.
- But we can represent by changing index into onehot encoded vector


```python
one_hot_3 = one_hot(3, n_users).float()

user_factors.t() @ one_hot_3
```




    tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])




```python
# 이거랑 같은 결과

user_factors[3]
```




    tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])



### Embedding

- 자료를 저장할때부터 벡터로 저장할필요는 없으니까 딥러닝에서 벡터로 변환하는것만 전담하는 한 층 있음 = Embedding

- 이미지 데이터의 RGB(3개의 요소)처럼 영화, 사용자 데이터 등은 단순하게 몇 개의 요소로 나타낼 수 없음 -> 모델이 알아서 learn하게끔 해야 함(개수만 정해주고 각 요소의 의미를 알아서 찾아가게끔) = Embedding

## collaborative filtering from scratch

### object-oriented programming

- things like python
- __ 붙은 녀석들은 특별한 의미가 있음: __init__의 경우에는 새로운 object가 생성된다는 의미
- module: we can add additional behavior to an existing class (superclass)
- forward: when module is called, Pytorch will call a method in class called forward, and it will pass along to that any parameters that are included in call.


```python
#yrange 고려
#사람마다 짜게 점수 주는 정도를 고려하고 싶지만 새로 나온 것에 대해서는 알 길이 없음 -> bias 없고 weight만 있음
#영화 자체가 좋거나 나쁜 정도를 고려하고 싶지만 새로 나온 것에 대해서는 알 길이 없음 -> bias 없고 weight만 있음
#그래서 bias도 embed함 (by user 개인의 전체 평균(짠지 후한지)과 전체 movie의 평균(그 영화가 좋은지 별로인지)를 더해줌)

class DotProductBias(Module):
    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):
        self.user_factors = Embedding(n_users, n_factors)
        self.user_bias = Embedding(n_users, 1)
        self.movie_factors = Embedding(n_movies, n_factors)
        self.movie_bias = Embedding(n_movies, 1)
        self.y_range = y_range
        
    def forward(self, x):
        users = self.user_factors(x[:,0])
        movies = self.movie_factors(x[:,1])
        res = (users * movies).sum(dim=1, keepdim=True)
        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])
        return sigmoid_range(res, *self.y_range)
```


```python
#overfit 돼서 일정 순간 이후에는 늘지 않는다

"""
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3)"""
```




    '\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)'



### weight decay: overfit 예방

- loss func에 sum of weight squared 더함 -> gradient 계산할 때 weight가 가능한 가장 작아지게 함 (conficients(=weight)가 클수록 loss function의 canyon이 뾰족해지기 때문에, 뾰족할수록 overfit 되니까)
- 따라서 loss_with_wd = loss +wd *(parameters**2).sum()
- (wd(weight decay)는 loss에 더하는 wieght 제곱들의 합)
- 그런데 파라미터 제곱합이 복잡하니까 파라미터에 대한 도함수로 접근 -> parameter.grad += wd*2*parameters


```python
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1) #wd 적용
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.921265</td>
      <td>0.941326</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.845829</td>
      <td>0.880891</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.740665</td>
      <td>0.835305</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.598242</td>
      <td>0.820787</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.509380</td>
      <td>0.820526</td>
      <td>00:21</td>
    </tr>
  </tbody>
</table>


### embedding module 직접 만들어보기

- module 만들 때 tensor와 parameter 모두 주기


```python
#안되는 버전


class T(Module):
    def __init__(self): self.a = torch.ones(3)

L(T().parameters())
```




    (#0) []




```python
#되는 버전(텐서와 파라미터 모두 주려면 텐서를 감싸줘야함)

class T(Module):
    def __init__(self): self.a = nn.Parameter(torch.ones(3))

L(T().parameters())
```




    (#1) [Parameter containing:
    tensor([1., 1., 1.], requires_grad=True)]




```python
#사실 모든 pytorch 모듈은 trainable한 parameter에 대해서 
#모두 nn.Para쓰고 있어서 겉으로 드러나게 써주지 않아도 됨

class T(Module):
    def __init__(self): self.a = nn.Linear(1, 3, bias=False)

t = T()
L(t.parameters())
```




    (#1) [Parameter containing:
    tensor([[-0.3874],
            [ 0.6574],
            [-0.7858]], requires_grad=True)]



이렇게 하고 나서 weight type 찍어보면 파라미터가 출력됨


```python
type(t.a.weight)
```




    torch.nn.parameter.Parameter



- random initialization이용해서 텐서를 파라미터로 사용 가능


```python
def create_params(size):
    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))
```

이것을 이용해 dotproductbias 다시 작업해보기


```python
class DotProductBias(Module):
    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):
        self.user_factors = create_params([n_users, n_factors])
        self.user_bias = create_params([n_users])
        self.movie_factors = create_params([n_movies, n_factors])
        self.movie_bias = create_params([n_movies])
        self.y_range = y_range
        
    def forward(self, x):
        users = self.user_factors[x[:,0]]
        movies = self.movie_factors[x[:,1]]
        res = (users*movies).sum(dim=1)
        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]
        return sigmoid_range(res, *self.y_range)
```


```python
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.946503</td>
      <td>0.950592</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.878636</td>
      <td>0.879906</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.715809</td>
      <td>0.841697</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.589360</td>
      <td>0.826072</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.510666</td>
      <td>0.826622</td>
      <td>00:22</td>
    </tr>
  </tbody>
</table>


### Interpreting Embeddings and Biases

- 의미 파악 어렵지 않음
- 평균적인 경향성을 평균과의 비교를 통해 실시 가능
- user는 PCA(principal component analysis)
- gpt 문의 결과 입력 (코드 해설, 산점도 그래프 그리는 것)


```python
movie_bias = learn.model.movie_bias.squeeze()
idxs = movie_bias.argsort()[:5]
[dls.classes['title'][i] for i in idxs]
```




    ['Leave It to Beaver (1997)',
     "Joe's Apartment (1996)",
     'Home Alone 3 (1997)',
     'Beautician and the Beast, The (1997)',
     'Free Willy 3: The Rescue (1997)']




```python
idxs = movie_bias.argsort(descending=True)[:5]
[dls.classes['title'][i] for i in idxs]
```




    ['Titanic (1997)',
     'L.A. Confidential (1997)',
     'Star Wars (1977)',
     'Fargo (1996)',
     "Schindler's List (1993)"]




```python
# 영화 평점 데이터를 처리하고 그 결과를 산점도로 시각화하는 작업

g = ratings.groupby('title')['rating'].count()
# g에 'ratings' 데이터프레임을 'title' 열을 기준으로 그룹화하고 각 영화의 평점 수를 카운트하여 저장

top_movies = g.sort_values(ascending=False).index.values[:1000]
# 내림차순 정렬해서 상위 1000개만 인덱스 가져옴

top_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies]) 
#.o2i[]는 []의 내용에 대해서 .앞의 항목에 대한 인덱스 끌어옴

movie_w = learn.model.movie_factors[top_idxs].cpu().detach()
#텐서에서 추출한 영화의 요소들을 cpu로 이동시키고 계산 그래프에서 분리하여 movie_m에 저장

movie_pca = movie_w.pca(3)
# 주성분 분석을 통해 차원을 3개로 축소

fac0,fac1,fac2 = movie_pca.t()
#3개의 주성분을 각각의 변수에 저장

idxs = list(range(50))
X = fac0[idxs]
Y = fac2[idxs]
# 두 개의 주성분에서 50개의 인덱스 설정하여 XY에 저장

plt.figure(figsize=(12,12))
plt.scatter(X, Y)
for i, x, y in zip(top_movies[idxs], X, Y):
    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
plt.show()
#두 개의 주성분으로 그림그리기
```


    
![png](/../../images/fastai_ch8/output_38_0.png)
    


## Using fastai.collab

- collab_learner 이용하여 collab filtering model 사용할 수 있음


```python
learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))
```


```python
learn.fit_one_cycle(5, 5e-3, wd=0.1)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.976881</td>
      <td>0.953688</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.846676</td>
      <td>0.878287</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.752395</td>
      <td>0.837353</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.588658</td>
      <td>0.825264</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.502694</td>
      <td>0.826392</td>
      <td>00:21</td>
    </tr>
  </tbody>
</table>



```python
learn.model
```




    EmbeddingDotBias(
      (u_weight): Embedding(944, 50)
      (i_weight): Embedding(1665, 50)
      (u_bias): Embedding(944, 1)
      (i_bias): Embedding(1665, 1)
    )



- .model 로 안에 들어있는 레이어 볼 수 있음
- 다른 코드에도 붙여서 쓸 수 있음

### Embedding distance


```python
movie_factors = learn.model.i_weight.weight
idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']
distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])
idx = distances.argsort(descending=True)[1]
dls.classes['title'][idx]
```




    'When We Were Kings (1996)'



- 50차원에서도 피타고라스의 정리 이용한 거리 계산 가능
- 코사인유사도(다른 영화들의 weight와 설정한 영화의 weight)

### Bootstrapping a collab filtering model

- 서비스 자체를 처음 시작하는 상태이거나 새로 가입한 사용자에게 추천할 때는 해결방법이 없음 ㅠㅠ
- 그래서 상식 선에서 해결하되, 해당 사용자의 평균적인 취향을 알아내야만 -> 여전히 초기 embedding vector 세우기 위해서는 사용자의 메타 데이터 기반의 tabular model 이용하는 것이 좋을듯 -> 질문에 대답하게 하여 signup 메타데이터 이용(물론 메타데이터 응답 시 너무 전부 다 취향이라고 하거나 전부 다 취향 아니라고 하는 사람들이 있을 수도 있음)
- 소수의 사람들이 추천 시스템의 방향을 설정하기 시작한다면, 그 사람들의 취향으로 점령당할것임, 기하급수적으로 증가할것임 -> 사람의 모니터링으로 해결

## Deep Learning for collab filtering


```python
embs = get_emb_sz(dls)
embs
```




    [(944, 74), (1665, 102)]




```python
class CollabNN(Module):
    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):
        self.user_factors = Embedding(*user_sz)
        self.item_factors = Embedding(*item_sz)
        self.layers = nn.Sequential(
            nn.Linear(user_sz[1]+item_sz[1], n_act),
            nn.ReLU(),
            nn.Linear(n_act, 1))
        self.y_range = y_range
        
    def forward(self, x):
        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) #! 위에있는x랑 밑에있는 x가 서로 다른건가..?
        x = self.layers(torch.cat(embs, dim=1)) #! 위에있는x랑 밑에있는 x가 서로 다른건가..?
        return sigmoid_range(x, *self.y_range)
```


```python
model = CollabNN(*embs)
```

- class CollabNN에다가 get_emb_sz(dls)로 저장한 적당한 크기의 embedding metrix 사이즈 전달해서 model에 저장


```python
"""learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.01)"""
```




    'learn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)'




```python
learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])
# use_nn 를 True로 하면 layers파라미터이용해서 추가 레이어 생성가능

learn.fit_one_cycle(5, 5e-3, wd=0.1)
```



<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.997382</td>
      <td>0.986980</td>
      <td>00:27</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.923815</td>
      <td>0.919812</td>
      <td>00:26</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.869044</td>
      <td>0.894629</td>
      <td>00:26</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.820195</td>
      <td>0.867804</td>
      <td>00:29</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.759989</td>
      <td>0.867344</td>
      <td>00:25</td>
    </tr>
  </tbody>
</table>


-learn.model은 EmbeddingNN의 object type이다


```python
learn.model
#hidden layer  진짜 있다
```




    EmbeddingNN(
      (embeds): ModuleList(
        (0): Embedding(944, 74)
        (1): Embedding(1665, 102)
      )
      (emb_drop): Dropout(p=0.0, inplace=False)
      (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layers): Sequential(
        (0): LinBnDrop(
          (0): Linear(in_features=176, out_features=100, bias=False)
          (1): ReLU(inplace=True)
          (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): LinBnDrop(
          (0): Linear(in_features=100, out_features=50, bias=False)
          (1): ReLU(inplace=True)
          (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): LinBnDrop(
          (0): Linear(in_features=50, out_features=1, bias=True)
        )
        (3): fastai.layers.SigmoidRange(low=0, high=5.5)
      )
    )




```python
@delegates(TabularModel)
class EmbeddingNN(TabularModel):
    def __init__(self, emb_szs, layers, **kwargs):
        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)
```

- 위의 코드 뭔소린지 모르겠다!
- kwargs는 모든 인수를 표 형식 모델에 다시 작성해야 하는 것을 방지하고 동기화를 유지하기 위해 nnEmbedding에 사용함
- 하지만 주피터노트북에서는 어떤 매개 변수를 사용할 수 있는지 모르기 때문에 API를 사용하기가 매우 어렵고 그래서 @delegates decorator 사용해서 문제 해결한다고 하는데,,, 왜 그런지는 모르게따!!!!
